{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reset-f\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.io import wb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\",category=UserWarning)\n",
    "from res_ind_lib import *\n",
    "import os, time\n",
    "import itertools\n",
    "\n",
    "from progress_reporter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Folder where outputs are stored\n",
    "out_folder = \"scorecards/\"\n",
    "os.makedirs(out_folder,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_original=pd.read_csv(\"df_original.csv\",index_col=\"country\")\n",
    "\n",
    "#appends \"countries\" from the sensitivity analysis\n",
    "df_original=df_original.append(pd.read_csv(\"df_sensit_input.csv\",index_col=\"country\"))\n",
    "\n",
    "df_with_results=pd.read_csv(\"df.csv\",index_col=\"country\")\n",
    "\n",
    "#Rankings \n",
    "ranks = df_with_results.dropna(how=\"all\",axis=1).dropna().rank(method=\"min\",ascending =False) \n",
    "ranks.to_csv(\"rankings.csv\")\n",
    "\n",
    "#appends sensitivity anlysis\n",
    "df_with_results=df_with_results.append(pd.read_csv(\"df_sensit_results.csv\").set_index(\"country\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#computes all derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deriv_set = np.setdiff1d( df_original.columns.values,\n",
    "    [\"pop\",\"iso3\",\"original_country\",\"gdp_pc_pp\",\"pov_head\",\"avg_prod_k_ref\",\"income_elast\",\"faref\",\"avg_prod_k\", \"peref\",\"vref\",\"share1_ref\"\n",
    "     ,\"bashs\",\"ophe\",\"fa_ref\",\"v_ref\"]+[c for c in df_original.columns if c.startswith(\"fa_ratio\")])\n",
    "pd.DataFrame(data=deriv_set).to_csv(\"deriv_set.csv\",index=False,header=False)\n",
    "\n",
    "def compute_derivative(df_original,outname):\n",
    "    der = pd.DataFrame()\n",
    "    h=0.0001\n",
    "    #loop on all data in df prior to add the results\n",
    "    fx = compute_resiliences(df_original)[outname]\n",
    "    for var in deriv_set:\n",
    "        try:\n",
    "            df_=df_original.copy(deep=True)\n",
    "            df_[var]=df_[var]+h\n",
    "            fxh= compute_resiliences(df_)[outname]\n",
    "            der[var] = (fxh-fx)/(h)\n",
    "        except TypeError:\n",
    "            print(\"no derivative for \" +var)\n",
    "    return der\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checks that info has information for all variables in deriv_set\n",
    "info = pd.read_csv(\"inputs_info.csv\").set_index(\"key\")\n",
    "for d in deriv_set:\n",
    "    if d not in info.index.values:\n",
    "        raise Exception(d+\" is not documented in inputs_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dKpc\n"
     ]
    }
   ],
   "source": [
    "#todo: save a loop by taking all score_card_set from the derivative\n",
    "\n",
    "#new dataframe with countries in rows and (resilience type,input) as column\n",
    "#score_card_set = [\"resilience\",\"resilience_no_shock\",\"resilience_no_shock_no_uspcale\",\"resilience_no_shock_no_SP\"] \n",
    "score_card_set = [\"resilience\",\"risk\",\"v_shew\", \"dKpc\"]\n",
    "headr = list(itertools.product(score_card_set,deriv_set))\n",
    "derivatives=  pd.DataFrame(index=df_original.dropna().index.values, columns=pd.MultiIndex.from_tuples(headr))\n",
    "\n",
    "#computes all derivatives\n",
    "for outname in score_card_set:\n",
    "    progress_reporter(outname)\n",
    "    derivatives[outname]=(compute_derivative(df_original,outname))/info[\"weight_der\"][deriv_set] #weights derivatives by number of people affected [avoids creating new clumns with nans]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computes derivative of risk wrt resilience\n",
    "der_risk = derivatives[\"risk\"].copy()\n",
    "der_risk[\"resilience\"]=(derivatives.risk/derivatives.resilience).mode(axis=1).mean(axis=1) #because of floating point operations, mode can return several close values\n",
    "\n",
    "der_vshew = derivatives[\"v_shew\"]\n",
    "der_risk[\"v_shew\"]=(der_risk.v/ (der_vshew.v)) \n",
    "\n",
    "derivatives[\"resilience\"].to_csv(\"deriv.csv\")\n",
    "der_risk.to_csv(\"deriv_risk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output signs in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saves derivatives in excel tabs with signs in colors (very usefull for understanding the model)\n",
    "writer= pd.ExcelWriter(\"signs.xlsx\", engine='xlsxwriter')\n",
    "workbook=writer.book\n",
    "# Add a format. Light red fill with dark red text.\n",
    "red = workbook.add_format({'bg_color': '#FFC7CE',\n",
    "                               'font_color': '#9C0006'})\n",
    "\n",
    "blue = workbook.add_format({'bg_color': '#92c5de',\n",
    "                               'font_color': '#000061'})\n",
    "for outname in score_card_set:\n",
    "#for outname in [\"resilience\"]:\n",
    "    (derivatives[outname].dropna()).to_excel(writer,sheet_name=outname)\n",
    "    writer.sheets[outname].conditional_format('B2:BB600', {'type':'cell',\n",
    "                                    'criteria': '>',\n",
    "                                    'value':    0,\n",
    "                                    'format':   blue})\n",
    "    writer.sheets[outname].conditional_format('B2:BB600', {'type':'cell',\n",
    "                                    'criteria': '<',\n",
    "                                    'value':    0,\n",
    "                                    'format':   red})\n",
    "    writer.sheets[outname].freeze_panes(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    writer.save()\n",
    "except PermissionError:\n",
    "    warnings.warn(\"Cannot write excel file. Check that it's not opened and try again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambigous sign for H\n",
      "ambigous sign for T_rebuild_K\n",
      "ambigous sign for pe\n",
      "ambigous sign for protection\n",
      "ambigous sign for pv\n",
      "ambigous sign for v\n"
     ]
    }
   ],
   "source": [
    "#Signs of resilience derivative \n",
    "der =     np.sign(derivatives[\"resilience\"]).replace(0,np.nan)\n",
    "signs= pd.Series(index=der.columns)\n",
    "for i in signs.index:\n",
    "    if (der[i].min()==der[i].max()): #all nonnan signs are equal\n",
    "        signs[i]=der[i].min()\n",
    "    else:\n",
    "        print(\"ambigous sign for \"+i)\n",
    "        signs[i]=np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write Excel scorecards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    abs_derivative = signs*derivatives[\"resilience\"] #abs value\n",
    "    step_for_one = (0.01/abs_derivative).replace([-np.inf,np.inf],[np.nan,np.nan])\n",
    "\n",
    "\n",
    "    headr = list(itertools.product(derivatives.dropna().index.values,['level', \"ranking\",'der','for_one']))\n",
    "    scores=  pd.DataFrame(index=deriv_set, columns=pd.MultiIndex.from_tuples(headr))\n",
    "\n",
    "\n",
    "    for c in derivatives.dropna().index:\n",
    "        scores[(c,'level')]=df_original.ix[c]\n",
    "        scores[(c,'der')]=abs_derivative.ix[c]\n",
    "        scores[(c,'for_one')]=step_for_one.ix[c]\n",
    "        scores[(c,'ranking')]=ranks.ix[df_original.ix[c,\"original_country\"]]\n",
    "\n",
    "\n",
    "    for c in derivatives.dropna().index:\n",
    "    #for c in [\"France\"]:\n",
    "        with pd.ExcelWriter('scorecards/'+c.lower().replace(\" \",\"_\").replace(\"\\\\\",\"\")+'.xlsx', engine='xlsxwriter') as writer:\n",
    "            percent = writer.book.add_format()\n",
    "            percent.set_num_format(\"0.0%\")\n",
    "\n",
    "            outs = df_with_results.ix[df_with_results.index==c,score_card_set]\n",
    "            outs.transpose().to_excel(writer)\n",
    "            scores[c].reset_index().rename(columns={\"index\":\"input\"}).to_excel(writer,startrow =2+len(score_card_set),index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_original.csv\").set_index(\"country\")[deriv_set].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=1, whiten=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.77598456e-17,   1.11022302e-16,   0.00000000e+00,\n",
       "         0.00000000e+00,  -2.73848559e-03,  -3.18713039e-03,\n",
       "        -3.12916269e-03,  -7.34183632e-05,  -2.85972244e-03,\n",
       "        -4.32485261e-17,  -4.32485261e-17,  -2.94918295e-03,\n",
       "        -2.46490729e-03,  -9.99934843e-01,   2.15032669e-04,\n",
       "        -6.76220843e-03,  -8.08985528e-05,  -3.17416685e-03,\n",
       "        -3.88388990e-03,  -2.05054954e-03,  -5.10451858e-04,\n",
       "         1.50803517e-03,   1.49139866e-03])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
